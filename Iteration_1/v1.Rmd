---
title: "Msc Dissertation"
output: html_notebook
---

Download the data
```{r}
# final edit
# download all metadata json files into google cloud storage bucket
library("plyr")
library("dplyr")
library("aws.s3")
library("purrr")
library("stringr")

setwd("/home/jupyter/goldencheetah_data")

Athlete_metadata <- get_bucket_df(bucket = "goldencheetah-opendata", prefix = "metadata", max = Inf) %>% pull(Key)

Athlete_metadata <- strsplit(Athlete_metadata, "\r\n")
Athlete_metadata = unlist(Athlete_metadata)
#str(Athlete_metadata)
#length(Athlete_metadata)
#Athlete_metadata[2:6607]
atm <- length(Athlete_metadata)
str_subset(Athlete_metadata[2:atm], "\\.zip$") %>%
  walk(function(key) {
    filename <- str_extract(key, "\\{.+")
    save_object(object = key, bucket = "goldencheetah-opendata", file = filename)
  })

t1 <- Sys.time()
# get all the zip files
zipF <- list.files(path = "/home/jupyter/goldencheetah_data", pattern = "*.zip", full.names = TRUE)

# unzip all your files
ldply(.data = zipF, .fun = unzip, exdir = "/home/jupyter/goldencheetah_data_unzipp")
t2 <- Sys.time()
print('Time to download and extract all files')
print(t2-t1)
```

Parse all the files into a bigquery table

```{r}
require(jsonlite)
require(data.table)
require(foreach)
require(doParallel)
# Get data into a single list

setwd("/home/jupyter/goldencheetah_data_unzipp/")
filenames <- list.files(path = "/home/jupyter/goldencheetah_data_unzipp/")
idx_bike <- list() # if we want to filter for "Bike" observations

registerDoParallel(cores = 16)
# registerDoSEQ()
t1 <- Sys.time()
idx_bike <- foreach(f = 1:6605, .combine = c, .multicombine = TRUE, .errorhandling = "remove") %dopar% {
  test <- try({
    which(fromJSON(txt=filenames[f])$RIDES$sport == "Bike" |
          fromJSON(txt=filenames[f])$RIDES$sport == "VirtualRide")
  })
  list(test)
}
t2 <- Sys.time()
paste0("time to run bike index loop")
print(t2 - t1)

dt_merged <- list()

dt_merged <- foreach(f = 4001:6605, .combine = c, .multicombine = TRUE, .errorhandling = "remove") %dopar% {
  test <- try({
    dt_merged <- fromJSON(txt=filenames[f])$RIDES$METRICS[idx_bike[[f]],]
    dt_merged[["date"]] <- fromJSON(txt=filenames[f])$RIDES$date[idx_bike[[f]]]
    dt_merged[["id"]] <-  as.list(filenames[f]) #rep(fromJSON(txt=filenames[f])$ATHLETE$id, length(dt_merged[[f]][["date"]]))
    dt_merged[["sport"]] <- fromJSON(txt=filenames[f])$RIDES$sport[idx_bike[[f]]]
    dt_merged[["yob"]] <-  fromJSON(txt=filenames[f])$ATHLETE$yob
    dt_merged[["gender"]] <-  fromJSON(txt=filenames[f])$ATHLETE$gender
    })
  list(dt_merged)
}

t3 <- Sys.time()
paste0("time to run dt_merged loop")
print(t3 - t2)



# Merge into a data.table
dt_merged <- rbindlist(dt_merged, fill = TRUE)
#head(dt_merged)

# Reorder columns such that "id", "date", "sport", "yob" and "gender" are the first columns
idx_metadata_cols <- which(colnames(dt_merged) %in% c("date","id","sport","yob","gender"))
idx_metric_cols <- which(!colnames(dt_merged) %in% c("date","id","sport","yob","gender"))
dt_merged <- dt_merged[,c(idx_metadata_cols,idx_metric_cols), with=FALSE]

#set all NA to 0 that are not in a list
#dt_merged[is.na(dt_merged)] <- 0
#dt_merged[is.null(dt_merged)] <- 0

idx_list <- which(sapply(dt_merged, class) == "list")
#idx_list

# select only columns required
dt_merged <- dt_merged[,c('date','id','sport','gender','yob','ride_count','workout_time','time_riding','athlete_weight','total_work','average_power','nonzero_power',
                          'max_power','cp_setting','coggan_np','coggan_if','coggan_tss','coggam_variability_index','coggan_tssperhour',
                          '1s_critical_power','5s_critical_power','10s_critical_power','15s_critical_power','20s_critical_power',
                          '30s_critical_power','1m_critical_power','2m_critical_power','3m_critical_power','5m_critical_power','8m_critical_power',
                          '10m_critical_power','20m_critical_power','30m_critical_power','60m_critical_power','time_in_zone_L1','time_in_zone_L2',
                          'time_in_zone_L3','time_in_zone_L4','time_in_zone_L5','time_in_zone_L6','time_in_zone_L7','1s_peak_wpk','5s_peak_wpk',
                          '10s_peak_wpk','15s_peak_wpk','20s_peak_wpk','30s_peak_wpk','1m_peak_wpk','5m_peak_wpk','10m_peak_wpk','20m_peak_wpk',
                          '30m_peak_wpk','60m_peak_wpk')]

# only include rows that have an average power value where a 1sec power value is not null
dt_merged <- dt_merged[!sapply(dt_merged$average_power, is.null)]
dt_merged <- dt_merged[!sapply(dt_merged$'1s_peak_wpk', is.null)]

# fix list columns that have null values
# remove rows with "NULL" values
zero_idx <- which(dt_merged$average_power=="NULL")
dt_merged$average_power[zero_idx] <- 0
dt_merged$average_power <- as.numeric(unlist(lapply(dt_merged$average_power, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$coggan_np=="NULL")
dt_merged$coggan_np[zero_idx] <- 0
dt_merged$coggan_np <- as.numeric(unlist(lapply(dt_merged$coggan_np, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$coggan_if=="NULL")
dt_merged$coggan_if[zero_idx] <- 0
dt_merged$coggan_if <- as.numeric(unlist(lapply(dt_merged$coggan_if, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$nonzero_power=="NULL")
dt_merged$nonzero_power[zero_idx] <- 0
dt_merged$nonzero_power <- as.numeric(unlist(lapply(dt_merged$nonzero_power, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$coggam_variability_index=="NULL")
dt_merged$coggam_variability_index[zero_idx] <- 0
dt_merged$coggam_variability_index <- as.numeric(unlist(lapply(dt_merged$coggam_variability_index, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$coggan_tssperhour=="NULL")
dt_merged$coggan_tssperhour[zero_idx] <- 0
dt_merged$coggan_tssperhour <- as.numeric(unlist(lapply(dt_merged$coggan_tssperhour, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$time_in_zone_L1=="NULL")
dt_merged$time_in_zone_L1[zero_idx] <- 0
dt_merged$time_in_zone_L1 <- as.numeric(unlist(lapply(dt_merged$time_in_zone_L1, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$time_in_zone_L2=="NULL")
dt_merged$time_in_zone_L2[zero_idx] <- 0
dt_merged$time_in_zone_L2 <- as.numeric(unlist(lapply(dt_merged$time_in_zone_L2, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$time_in_zone_L3=="NULL")
dt_merged$time_in_zone_L3[zero_idx] <- 0
dt_merged$time_in_zone_L3 <- as.numeric(unlist(lapply(dt_merged$time_in_zone_L3, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$time_in_zone_L4=="NULL")
dt_merged$time_in_zone_L4[zero_idx] <- 0
dt_merged$time_in_zone_L4 <- as.numeric(unlist(lapply(dt_merged$time_in_zone_L4, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$time_in_zone_L5=="NULL")
dt_merged$time_in_zone_L5[zero_idx] <- 0
dt_merged$time_in_zone_L5 <- as.numeric(unlist(lapply(dt_merged$time_in_zone_L5, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$time_in_zone_L6=="NULL")
dt_merged$time_in_zone_L6[zero_idx] <- 0
dt_merged$time_in_zone_L6 <- as.numeric(unlist(lapply(dt_merged$time_in_zone_L6, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$time_in_zone_L7=="NULL")
dt_merged$time_in_zone_L7[zero_idx] <- 0
dt_merged$time_in_zone_L7 <- as.numeric(unlist(lapply(dt_merged$time_in_zone_L7, `[[`, 1)))

# change column types
#dt_merged$date <- as.POSIXct(dt_merged$date, "%Y-%m-%d %H:%M:%S")
dt_merged$ride_count <- as.numeric(dt_merged$ride_count)
dt_merged$workout_time <- as.numeric(dt_merged$workout_time)
dt_merged$time_riding <- as.numeric(dt_merged$time_riding)
dt_merged$athlete_weight <- as.numeric(dt_merged$athlete_weight)
dt_merged$total_work <- as.numeric(dt_merged$total_work)
dt_merged$max_power <- as.numeric(dt_merged$max_power)
dt_merged$cp_setting <- as.numeric(dt_merged$cp_setting)
dt_merged$coggan_tss <- as.numeric(dt_merged$coggan_tss)
dt_merged$'1s_critical_power' <- as.numeric(dt_merged$'1s_critical_power')
dt_merged$'5s_critical_power' <- as.numeric(dt_merged$'5s_critical_power')
dt_merged$'10s_critical_power' <- as.numeric(dt_merged$'10s_critical_power')
dt_merged$'15s_critical_power' <- as.numeric(dt_merged$'15s_critical_power')
dt_merged$'20s_critical_power' <- as.numeric(dt_merged$'20s_critical_power')
dt_merged$'30s_critical_power' <- as.numeric(dt_merged$'30s_critical_power')
dt_merged$'1m_critical_power' <- as.numeric(dt_merged$'1m_critical_power')
dt_merged$'2m_critical_power' <- as.numeric(dt_merged$'2m_critical_power')
dt_merged$'3m_critical_power' <- as.numeric(dt_merged$'3m_critical_power')
dt_merged$'5m_critical_power' <- as.numeric(dt_merged$'5m_critical_power')
dt_merged$'8m_critical_power' <- as.numeric(dt_merged$'8m_critical_power')
dt_merged$'10m_critical_power' <- as.numeric(dt_merged$'10m_critical_power')
dt_merged$'20m_critical_power' <- as.numeric(dt_merged$'20m_critical_power')
dt_merged$'30m_critical_power' <- as.numeric(dt_merged$'30m_critical_power')
dt_merged$'60m_critical_power' <- as.numeric(dt_merged$'60m_critical_power')
dt_merged$time_in_zone_L1 <- as.numeric(dt_merged$time_in_zone_L1)
dt_merged$time_in_zone_L2 <- as.numeric(dt_merged$time_in_zone_L2)
dt_merged$time_in_zone_L3 <- as.numeric(dt_merged$time_in_zone_L3)
dt_merged$time_in_zone_L4 <- as.numeric(dt_merged$time_in_zone_L4)
dt_merged$time_in_zone_L5 <- as.numeric(dt_merged$time_in_zone_L5)
dt_merged$time_in_zone_L6 <- as.numeric(dt_merged$time_in_zone_L6)
dt_merged$time_in_zone_L7 <- as.numeric(dt_merged$time_in_zone_L7)
dt_merged$'1s_peak_wpk' <- as.numeric(dt_merged$'1s_peak_wpk')
dt_merged$'5s_peak_wpk' <- as.numeric(dt_merged$'5s_peak_wpk')
dt_merged$'10s_peak_wpk' <- as.numeric(dt_merged$'10s_peak_wpk')
dt_merged$'15s_peak_wpk' <- as.numeric(dt_merged$'15s_peak_wpk')
dt_merged$'20s_peak_wpk' <- as.numeric(dt_merged$'20s_peak_wpk')
dt_merged$'30s_peak_wpk' <- as.numeric(dt_merged$'30s_peak_wpk')
dt_merged$'1m_peak_wpk' <- as.numeric(dt_merged$'1m_peak_wpk')
dt_merged$'5m_peak_wpk' <- as.numeric(dt_merged$'5m_peak_wpk')
dt_merged$'10m_peak_wpk' <- as.numeric(dt_merged$'10m_peak_wpk')
dt_merged$'20m_peak_wpk' <- as.numeric(dt_merged$'20m_peak_wpk')
dt_merged$'30m_peak_wpk' <- as.numeric(dt_merged$'30m_peak_wpk')
dt_merged$'60m_peak_wpk' <- as.numeric(dt_merged$'60m_peak_wpk')
#class(dt_merged$coggan_tssperhour)
# rename columns that start with a number for BigQuery upload
setnames(dt_merged, old=c('1s_critical_power'), new=c('critical_power_1s'))
setnames(dt_merged, old=c('5s_critical_power'), new=c('critical_power_5s'))
setnames(dt_merged, old=c('10s_critical_power'), new=c('critical_power_10s'))
setnames(dt_merged, old=c('15s_critical_power'), new=c('critical_power_15s'))
setnames(dt_merged, old=c('20s_critical_power'), new=c('critical_power_20s'))
setnames(dt_merged, old=c('30s_critical_power'), new=c('critical_power_30s'))
setnames(dt_merged, old=c('1m_critical_power'), new=c('critical_power_1m'))
setnames(dt_merged, old=c('2m_critical_power'), new=c('critical_power_2m'))
setnames(dt_merged, old=c('3m_critical_power'), new=c('critical_power_3m'))
setnames(dt_merged, old=c('5m_critical_power'), new=c('critical_power_5m'))
setnames(dt_merged, old=c('8m_critical_power'), new=c('critical_power_8m'))
setnames(dt_merged, old=c('10m_critical_power'), new=c('critical_power_10m'))
setnames(dt_merged, old=c('20m_critical_power'), new=c('critical_power_20m'))
setnames(dt_merged, old=c('30m_critical_power'), new=c('critical_power_30m'))
setnames(dt_merged, old=c('60m_critical_power'), new=c('critical_power_60m'))
setnames(dt_merged, old=c('1s_peak_wpk'), new=c('peak_wpk_1s'))
setnames(dt_merged, old=c('5s_peak_wpk'), new=c('peak_wpk_5s'))
setnames(dt_merged, old=c('10s_peak_wpk'), new=c('peak_wpk_10s'))
setnames(dt_merged, old=c('15s_peak_wpk'), new=c('peak_wpk_15s'))
setnames(dt_merged, old=c('20s_peak_wpk'), new=c('peak_wpk_20s'))
setnames(dt_merged, old=c('30s_peak_wpk'), new=c('peak_wpk_30s'))
setnames(dt_merged, old=c('1m_peak_wpk'), new=c('peak_wpk_1m'))
setnames(dt_merged, old=c('5m_peak_wpk'), new=c('peak_wpk_5m'))
setnames(dt_merged, old=c('10m_peak_wpk'), new=c('peak_wpk_10m'))
setnames(dt_merged, old=c('20m_peak_wpk'), new=c('peak_wpk_20m'))
setnames(dt_merged, old=c('30m_peak_wpk'), new=c('peak_wpk_30m'))
setnames(dt_merged, old=c('60m_peak_wpk'), new=c('peak_wpk_60m'))

dt_merged$id <- unlist(dt_merged$id)

#idx_list <- which(sapply(dt_merged, class)=="list") # this is empty, implying no more list types

t4 <- Sys.time()
paste0('Time to subset list elements, convert data types')
print(t4-t3)
  

    
#write to bigquery
t3 <- Sys.time()
library(bigQueryR)
# use this to upload data to bigquery table
bqr_auth()
bqr_upload_data(projectId = "xxx", 
                datasetId = "goldencheetah_metadata",#name of gcp projectid
                tableId = "all_metadata_4001_6605", #name of the table 
                upload_data = dt_merged) #where the data is coming from
t4 <- Sys.time()
paste0('Uploaded to BigQuery')
print(t3-t4)
```
Imported exported bigquery data into R
convert to datatable and summarise all fields
```{r}
library(bigrquery)
projectid <- "xxx"
query <-
"
create table `algebraic-pier-304102.goldencheetah_metadata.all_metadata_summary` as
SELECT
    a.id,
    a.gender,
    a.yob,
    round(sum(ride_count),2) as no_of_rides,
    round(avg(workout_time),2) as avg_workout_time,
    round(avg(time_riding),2) as avg_time_riding,
    round(avg(athlete_weight),2) as avg_weight,
    round(avg(a.average_power),2) as avg_power_all_rides,
    round(avg(a.nonzero_power),2) as avg_nonzero_power_all_rides,
    round(max(max_power),2) as max_power,
    round(max(cp_setting),2) as best_cp_setting,
    round(avg(coggan_np),2) as avg_coggan_np,
    round(avg(coggam_variability_index),2) as avg_coggan_variability_index,
    round(max(critical_power_1s),2) as max_critical_power_1s,	
    round(max(critical_power_5s),2) as max_critical_power_5s,
    round(max(critical_power_10s),2) as max_critical_power_10s,
    round(max(critical_power_15s),2) as max_critical_power_15s,	
    round(max(critical_power_20s),2) as max_critical_power_20s,
    round(max(critical_power_30s),2) as max_critical_power_30s,	
    round(max(critical_power_1m),2) as max_critical_power_1m,
    round(max(critical_power_2m),2) as max_critical_power_2m,
    round(max(critical_power_3m),2) as max_critical_power_3m,
    round(max(critical_power_5m),2) as max_critical_power_5m,
    round(max(critical_power_8m),2) as max_critical_power_8m,
    round(max(critical_power_10m),2) as max_critical_power_10m,
    round(max(critical_power_20m),2) as max_critical_power_20m,
    round(max(critical_power_30m),2) as max_critical_power_30m,
    round(max(critical_power_60m),2) as max_critical_power_60m,
    round(avg(time_in_zone_L1),2) as avg_time_in_zone_L1,
    round(avg(time_in_zone_L2),2) as avg_time_in_zone_L2,
    round(avg(time_in_zone_L3),2) as avg_time_in_zone_L3,
    round(avg(time_in_zone_L4),2) as avg_time_in_zone_L4,
    round(avg(time_in_zone_L5),2) as avg_time_in_zone_L5,
    round(avg(time_in_zone_L6),2) as avg_time_in_zone_L6,
    round(avg(time_in_zone_L7),2) as avg_time_in_zone_L7,
    round(max(peak_wpk_1s),2) as max_peak_wpk_1s,
    round(max(peak_wpk_5s),2) as max_peak_wpk_5s,
    round(max(peak_wpk_10s),2) as max_peak_wpk_10s,	
    round(max(peak_wpk_15s),2) as max_peak_wpk_15s,	
    round(max(peak_wpk_20s),2) as max_peak_wpk_20s,	
    round(max(peak_wpk_30s),2) as max_peak_wpk_30s,	
    round(max(peak_wpk_1m),2) as max_peak_wpk_1m,
    round(max(peak_wpk_5m),2) as max_peak_wpk_5m,
    round(max(peak_wpk_10m),2) as max_peak_wpk_10m,	
    round(max(peak_wpk_20m),2) as max_peak_wpk_20m,	
    round(max(peak_wpk_30m),2) as max_peak_wpk_30m,	
    round(max(peak_wpk_60m),2) as max_peak_wpk_60m
    

FROM 
    `algebraic-pier-304102.goldencheetah_metadata.all_metadata` a
--WHERE
--   a.id = '{4ed32d19-aeb6-43c1-aa11-7c41a1e935ee}.json' 
GROUP BY
   a.id,
   a.yob,
   a.gender
"
# Run the query
metadata_all_summary <- query_exec(query, projectid, use_legacy_sql = FALSE)
```
The next portion was not run in a jupyter notebook, but rather Rstudio, as the final output file was now small enough to run on local machine. It was therefore export to Google Cloud Storage in CSV format, downloaded onto my local machine and uploaded in Rstudio
