---
title: "Msc Dissertation"
output: html_notebook
---

Download the data
```{r}
# final edit
# download all metadata json files into Google Cloud Storage Bucket
library("plyr")
library("dplyr")
library("aws.s3")
library("purrr")
library("stringr")

setwd("/home/jupyter/goldencheetah_data")

Athlete_metadata <- get_bucket_df(bucket = "goldencheetah-opendata", prefix = "metadata", max = Inf) %>% pull(Key)

Athlete_metadata <- strsplit(Athlete_metadata, "\r\n")
Athlete_metadata = unlist(Athlete_metadata)
#str(Athlete_metadata)
#length(Athlete_metadata)
#Athlete_metadata[2:6607]
atm <- length(Athlete_metadata)
str_subset(Athlete_metadata[2:atm], "\\.zip$") %>%
  walk(function(key) {
    filename <- str_extract(key, "\\{.+")
    save_object(object = key, bucket = "goldencheetah-opendata", file = filename)
  })

t1 <- Sys.time()
# get all the zip files
zipF <- list.files(path = "/home/jupyter/goldencheetah_data", pattern = "*.zip", full.names = TRUE)

# unzip all your files
ldply(.data = zipF, .fun = unzip, exdir = "/home/jupyter/goldencheetah_data_unzipp")
t2 <- Sys.time()
print('Time to download and extract all files')
print(t2-t1)
```
Parse all the files into a BigQuery table
```{r}
require(jsonlite)
require(data.table)
require(foreach)
require(doParallel)
# get data into a single list

setwd("/home/jupyter/goldencheetah_data_unzipp/")
filenames <- list.files(path = "/home/jupyter/goldencheetah_data_unzipp/")
idx_bike <- list() # if we want to filter for "Bike" observations

registerDoParallel(cores = 16)
# registerDoSEQ()
t1 <- Sys.time()
idx_bike <- foreach(f = 1:6605, .combine = c, .multicombine = TRUE, .errorhandling = "remove") %dopar% {
  test <- try({
    which(fromJSON(txt=filenames[f])$RIDES$sport == "Bike" |
          fromJSON(txt=filenames[f])$RIDES$sport == "VirtualRide")
  })
  list(test)
}
t2 <- Sys.time()
paste0("time to run bike index loop")
print(t2 - t1)

dt_merged <- list()

dt_merged <- foreach(f = 4001:6605, .combine = c, .multicombine = TRUE, .errorhandling = "remove") %dopar% {
  test <- try({
    dt_merged <- fromJSON(txt=filenames[f])$RIDES$METRICS[idx_bike[[f]],]
    dt_merged[["date"]] <- fromJSON(txt=filenames[f])$RIDES$date[idx_bike[[f]]]
    dt_merged[["id"]] <-  as.list(filenames[f]) #rep(fromJSON(txt=filenames[f])$ATHLETE$id, length(dt_merged[[f]][["date"]]))
    dt_merged[["sport"]] <- fromJSON(txt=filenames[f])$RIDES$sport[idx_bike[[f]]]
    dt_merged[["yob"]] <-  fromJSON(txt=filenames[f])$ATHLETE$yob
    dt_merged[["gender"]] <-  fromJSON(txt=filenames[f])$ATHLETE$gender
    })
  list(dt_merged)
}

t3 <- Sys.time()
paste0("time to run dt_merged loop")
print(t3 - t2)



# merge into a data.table
dt_merged <- rbindlist(dt_merged, fill = TRUE)
#head(dt_merged)

# reorder columns such that "id", "date", "sport", "yob" and "gender" are the first columns
idx_metadata_cols <- which(colnames(dt_merged) %in% c("date","id","sport","yob","gender"))
idx_metric_cols <- which(!colnames(dt_merged) %in% c("date","id","sport","yob","gender"))
dt_merged <- dt_merged[,c(idx_metadata_cols,idx_metric_cols), with=FALSE]

# set all NA to 0 that are not in a list
# dt_merged[is.na(dt_merged)] <- 0
# dt_merged[is.null(dt_merged)] <- 0

idx_list <- which(sapply(dt_merged, class) == "list")
# idx_list

# select only columns required
dt_merged <- dt_merged[,c('date','id','sport','gender','yob','ride_count','workout_time','time_riding','athlete_weight','total_work','average_power','nonzero_power',
                          'max_power','cp_setting','coggan_np','coggan_if','coggan_tss','coggam_variability_index','coggan_tssperhour',
                          '1s_critical_power','5s_critical_power','10s_critical_power','15s_critical_power','20s_critical_power',
                          '30s_critical_power','1m_critical_power','2m_critical_power','3m_critical_power','5m_critical_power','8m_critical_power',
                          '10m_critical_power','20m_critical_power','30m_critical_power','60m_critical_power','time_in_zone_L1','time_in_zone_L2',
                          'time_in_zone_L3','time_in_zone_L4','time_in_zone_L5','time_in_zone_L6','time_in_zone_L7','1s_peak_wpk','5s_peak_wpk',
                          '10s_peak_wpk','15s_peak_wpk','20s_peak_wpk','30s_peak_wpk','1m_peak_wpk','5m_peak_wpk','10m_peak_wpk','20m_peak_wpk',
                          '30m_peak_wpk','60m_peak_wpk')]

# only include rows that have an average power value where a 1sec power value is not null
dt_merged <- dt_merged[!sapply(dt_merged$average_power, is.null)]
dt_merged <- dt_merged[!sapply(dt_merged$'1s_peak_wpk', is.null)]

# fix list columns that have null values
# remove rows with "NULL" values
zero_idx <- which(dt_merged$average_power=="NULL")
dt_merged$average_power[zero_idx] <- 0
dt_merged$average_power <- as.numeric(unlist(lapply(dt_merged$average_power, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$coggan_np=="NULL")
dt_merged$coggan_np[zero_idx] <- 0
dt_merged$coggan_np <- as.numeric(unlist(lapply(dt_merged$coggan_np, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$coggan_if=="NULL")
dt_merged$coggan_if[zero_idx] <- 0
dt_merged$coggan_if <- as.numeric(unlist(lapply(dt_merged$coggan_if, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$nonzero_power=="NULL")
dt_merged$nonzero_power[zero_idx] <- 0
dt_merged$nonzero_power <- as.numeric(unlist(lapply(dt_merged$nonzero_power, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$coggam_variability_index=="NULL")
dt_merged$coggam_variability_index[zero_idx] <- 0
dt_merged$coggam_variability_index <- as.numeric(unlist(lapply(dt_merged$coggam_variability_index, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$coggan_tssperhour=="NULL")
dt_merged$coggan_tssperhour[zero_idx] <- 0
dt_merged$coggan_tssperhour <- as.numeric(unlist(lapply(dt_merged$coggan_tssperhour, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$time_in_zone_L1=="NULL")
dt_merged$time_in_zone_L1[zero_idx] <- 0
dt_merged$time_in_zone_L1 <- as.numeric(unlist(lapply(dt_merged$time_in_zone_L1, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$time_in_zone_L2=="NULL")
dt_merged$time_in_zone_L2[zero_idx] <- 0
dt_merged$time_in_zone_L2 <- as.numeric(unlist(lapply(dt_merged$time_in_zone_L2, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$time_in_zone_L3=="NULL")
dt_merged$time_in_zone_L3[zero_idx] <- 0
dt_merged$time_in_zone_L3 <- as.numeric(unlist(lapply(dt_merged$time_in_zone_L3, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$time_in_zone_L4=="NULL")
dt_merged$time_in_zone_L4[zero_idx] <- 0
dt_merged$time_in_zone_L4 <- as.numeric(unlist(lapply(dt_merged$time_in_zone_L4, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$time_in_zone_L5=="NULL")
dt_merged$time_in_zone_L5[zero_idx] <- 0
dt_merged$time_in_zone_L5 <- as.numeric(unlist(lapply(dt_merged$time_in_zone_L5, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$time_in_zone_L6=="NULL")
dt_merged$time_in_zone_L6[zero_idx] <- 0
dt_merged$time_in_zone_L6 <- as.numeric(unlist(lapply(dt_merged$time_in_zone_L6, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$time_in_zone_L7=="NULL")
dt_merged$time_in_zone_L7[zero_idx] <- 0
dt_merged$time_in_zone_L7 <- as.numeric(unlist(lapply(dt_merged$time_in_zone_L7, `[[`, 1)))

# change column types
# dt_merged$date <- as.POSIXct(dt_merged$date, "%Y-%m-%d %H:%M:%S")
dt_merged$ride_count <- as.numeric(dt_merged$ride_count)
dt_merged$workout_time <- as.numeric(dt_merged$workout_time)
dt_merged$time_riding <- as.numeric(dt_merged$time_riding)
dt_merged$athlete_weight <- as.numeric(dt_merged$athlete_weight)
dt_merged$total_work <- as.numeric(dt_merged$total_work)
dt_merged$max_power <- as.numeric(dt_merged$max_power)
dt_merged$cp_setting <- as.numeric(dt_merged$cp_setting)
dt_merged$coggan_tss <- as.numeric(dt_merged$coggan_tss)
dt_merged$'1s_critical_power' <- as.numeric(dt_merged$'1s_critical_power')
dt_merged$'5s_critical_power' <- as.numeric(dt_merged$'5s_critical_power')
dt_merged$'10s_critical_power' <- as.numeric(dt_merged$'10s_critical_power')
dt_merged$'15s_critical_power' <- as.numeric(dt_merged$'15s_critical_power')
dt_merged$'20s_critical_power' <- as.numeric(dt_merged$'20s_critical_power')
dt_merged$'30s_critical_power' <- as.numeric(dt_merged$'30s_critical_power')
dt_merged$'1m_critical_power' <- as.numeric(dt_merged$'1m_critical_power')
dt_merged$'2m_critical_power' <- as.numeric(dt_merged$'2m_critical_power')
dt_merged$'3m_critical_power' <- as.numeric(dt_merged$'3m_critical_power')
dt_merged$'5m_critical_power' <- as.numeric(dt_merged$'5m_critical_power')
dt_merged$'8m_critical_power' <- as.numeric(dt_merged$'8m_critical_power')
dt_merged$'10m_critical_power' <- as.numeric(dt_merged$'10m_critical_power')
dt_merged$'20m_critical_power' <- as.numeric(dt_merged$'20m_critical_power')
dt_merged$'30m_critical_power' <- as.numeric(dt_merged$'30m_critical_power')
dt_merged$'60m_critical_power' <- as.numeric(dt_merged$'60m_critical_power')
dt_merged$time_in_zone_L1 <- as.numeric(dt_merged$time_in_zone_L1)
dt_merged$time_in_zone_L2 <- as.numeric(dt_merged$time_in_zone_L2)
dt_merged$time_in_zone_L3 <- as.numeric(dt_merged$time_in_zone_L3)
dt_merged$time_in_zone_L4 <- as.numeric(dt_merged$time_in_zone_L4)
dt_merged$time_in_zone_L5 <- as.numeric(dt_merged$time_in_zone_L5)
dt_merged$time_in_zone_L6 <- as.numeric(dt_merged$time_in_zone_L6)
dt_merged$time_in_zone_L7 <- as.numeric(dt_merged$time_in_zone_L7)
dt_merged$'1s_peak_wpk' <- as.numeric(dt_merged$'1s_peak_wpk')
dt_merged$'5s_peak_wpk' <- as.numeric(dt_merged$'5s_peak_wpk')
dt_merged$'10s_peak_wpk' <- as.numeric(dt_merged$'10s_peak_wpk')
dt_merged$'15s_peak_wpk' <- as.numeric(dt_merged$'15s_peak_wpk')
dt_merged$'20s_peak_wpk' <- as.numeric(dt_merged$'20s_peak_wpk')
dt_merged$'30s_peak_wpk' <- as.numeric(dt_merged$'30s_peak_wpk')
dt_merged$'1m_peak_wpk' <- as.numeric(dt_merged$'1m_peak_wpk')
dt_merged$'5m_peak_wpk' <- as.numeric(dt_merged$'5m_peak_wpk')
dt_merged$'10m_peak_wpk' <- as.numeric(dt_merged$'10m_peak_wpk')
dt_merged$'20m_peak_wpk' <- as.numeric(dt_merged$'20m_peak_wpk')
dt_merged$'30m_peak_wpk' <- as.numeric(dt_merged$'30m_peak_wpk')
dt_merged$'60m_peak_wpk' <- as.numeric(dt_merged$'60m_peak_wpk')
# class(dt_merged$coggan_tssperhour)
# rename columns that start with a number for BigQuery upload
setnames(dt_merged, old=c('1s_critical_power'), new=c('critical_power_1s'))
setnames(dt_merged, old=c('5s_critical_power'), new=c('critical_power_5s'))
setnames(dt_merged, old=c('10s_critical_power'), new=c('critical_power_10s'))
setnames(dt_merged, old=c('15s_critical_power'), new=c('critical_power_15s'))
setnames(dt_merged, old=c('20s_critical_power'), new=c('critical_power_20s'))
setnames(dt_merged, old=c('30s_critical_power'), new=c('critical_power_30s'))
setnames(dt_merged, old=c('1m_critical_power'), new=c('critical_power_1m'))
setnames(dt_merged, old=c('2m_critical_power'), new=c('critical_power_2m'))
setnames(dt_merged, old=c('3m_critical_power'), new=c('critical_power_3m'))
setnames(dt_merged, old=c('5m_critical_power'), new=c('critical_power_5m'))
setnames(dt_merged, old=c('8m_critical_power'), new=c('critical_power_8m'))
setnames(dt_merged, old=c('10m_critical_power'), new=c('critical_power_10m'))
setnames(dt_merged, old=c('20m_critical_power'), new=c('critical_power_20m'))
setnames(dt_merged, old=c('30m_critical_power'), new=c('critical_power_30m'))
setnames(dt_merged, old=c('60m_critical_power'), new=c('critical_power_60m'))
setnames(dt_merged, old=c('1s_peak_wpk'), new=c('peak_wpk_1s'))
setnames(dt_merged, old=c('5s_peak_wpk'), new=c('peak_wpk_5s'))
setnames(dt_merged, old=c('10s_peak_wpk'), new=c('peak_wpk_10s'))
setnames(dt_merged, old=c('15s_peak_wpk'), new=c('peak_wpk_15s'))
setnames(dt_merged, old=c('20s_peak_wpk'), new=c('peak_wpk_20s'))
setnames(dt_merged, old=c('30s_peak_wpk'), new=c('peak_wpk_30s'))
setnames(dt_merged, old=c('1m_peak_wpk'), new=c('peak_wpk_1m'))
setnames(dt_merged, old=c('5m_peak_wpk'), new=c('peak_wpk_5m'))
setnames(dt_merged, old=c('10m_peak_wpk'), new=c('peak_wpk_10m'))
setnames(dt_merged, old=c('20m_peak_wpk'), new=c('peak_wpk_20m'))
setnames(dt_merged, old=c('30m_peak_wpk'), new=c('peak_wpk_30m'))
setnames(dt_merged, old=c('60m_peak_wpk'), new=c('peak_wpk_60m'))

dt_merged$id <- unlist(dt_merged$id)

# idx_list <- which(sapply(dt_merged, class)=="list") # this is empty, implying no more list types

t4 <- Sys.time()
paste0('Time to subset list elements, convert data types')
print(t4-t3)
  

    
# write to bigquery
t3 <- Sys.time()
library(bigQueryR)
# use this to upload data to bigquery table
bqr_auth()
bqr_upload_data(projectId = "xxx", 
                datasetId = "goldencheetah_metadata",#name of gcp projectid
                tableId = "all_metadata", #name of the table 
                upload_data = dt_merged) #where the data is coming from
t4 <- Sys.time()
paste0('Uploaded to BigQuery')
print(t3-t4)
```
Imported exported BigQuery data into R
Convert to datatable and summarise all fields
```{r}
# clean data from power meter errors and unrealistic numbers
library(bigrquery)
projectid <- "xxx"
query1 <-
"
create or replace table `xxxx.goldencheetah_metadata.all_metadata_clean` as
-- drop rides that have numbers that out of realm of possibility based on the table found on pg 41 (power profile chart)
-- max values will come from the men
-- athlete min age 16
-- athlete max age 100
-- weight between 50-150kgs
SELECT  
    *
FROM 
    `algebraic-pier-304102.goldencheetah_metadata.all_metadata` a
WHERE
    a.critical_power_5s/athlete_weight <= 25.18 --and 7.09
    and a.critical_power_1m/athlete_weight <= 11.5 --and 4.1
    and a.critical_power_5m/athlete_weight <= 7.6 --and 1.29
    and a.critical_power_60m/athlete_weight <= 6.6 --and 0.98
    and a.athlete_weight between 50 and 150
    and cast(a.yob as numeric) between 1920 and 2004
"
# run the query
metadata_all_clean <- query_exec(query1, projectid, use_legacy_sql = FALSE)
```
Write all_metadata_summary table to BigQuery
```{r}
library(bigrquery)
projectid <- "xxx"
query <-
"
create table `algebraic-pier-304102.goldencheetah_metadata.all_metadata_summary` as
SELECT
    a.id,
    a.gender,
    a.yob,
    round(sum(ride_count),2) as no_of_rides,
    round(avg(workout_time),2) as avg_workout_time,
    round(avg(time_riding),2) as avg_time_riding,
    round(avg(athlete_weight),2) as avg_weight,
    round(avg(a.average_power),2) as avg_power_all_rides,
    round(avg(a.nonzero_power),2) as avg_nonzero_power_all_rides,
    round(max(max_power),2) as max_power,
    round(max(cp_setting),2) as best_cp_setting,
    round(avg(coggan_np),2) as avg_coggan_np,
    round(avg(coggam_variability_index),2) as avg_coggan_variability_index,
    round(max(critical_power_1s),2) as max_critical_power_1s,	
    round(max(critical_power_5s),2) as max_critical_power_5s,
    round(max(critical_power_10s),2) as max_critical_power_10s,
    round(max(critical_power_15s),2) as max_critical_power_15s,	
    round(max(critical_power_20s),2) as max_critical_power_20s,
    round(max(critical_power_30s),2) as max_critical_power_30s,	
    round(max(critical_power_1m),2) as max_critical_power_1m,
    round(max(critical_power_2m),2) as max_critical_power_2m,
    round(max(critical_power_3m),2) as max_critical_power_3m,
    round(max(critical_power_5m),2) as max_critical_power_5m,
    round(max(critical_power_8m),2) as max_critical_power_8m,
    round(max(critical_power_10m),2) as max_critical_power_10m,
    round(max(critical_power_20m),2) as max_critical_power_20m,
    round(max(critical_power_30m),2) as max_critical_power_30m,
    round(max(critical_power_60m),2) as max_critical_power_60m,
    round(avg(time_in_zone_L1),2) as avg_time_in_zone_L1,
    round(avg(time_in_zone_L2),2) as avg_time_in_zone_L2,
    round(avg(time_in_zone_L3),2) as avg_time_in_zone_L3,
    round(avg(time_in_zone_L4),2) as avg_time_in_zone_L4,
    round(avg(time_in_zone_L5),2) as avg_time_in_zone_L5,
    round(avg(time_in_zone_L6),2) as avg_time_in_zone_L6,
    round(avg(time_in_zone_L7),2) as avg_time_in_zone_L7,
    round(max(peak_wpk_1s),2) as max_peak_wpk_1s,
    round(max(peak_wpk_5s),2) as max_peak_wpk_5s,
    round(max(peak_wpk_10s),2) as max_peak_wpk_10s,	
    round(max(peak_wpk_15s),2) as max_peak_wpk_15s,	
    round(max(peak_wpk_20s),2) as max_peak_wpk_20s,	
    round(max(peak_wpk_30s),2) as max_peak_wpk_30s,	
    round(max(peak_wpk_1m),2) as max_peak_wpk_1m,
    round(max(peak_wpk_5m),2) as max_peak_wpk_5m,
    round(max(peak_wpk_10m),2) as max_peak_wpk_10m,	
    round(max(peak_wpk_20m),2) as max_peak_wpk_20m,	
    round(max(peak_wpk_30m),2) as max_peak_wpk_30m,	
    round(max(peak_wpk_60m),2) as max_peak_wpk_60m
    

FROM 
    `algebraic-pier-304102.goldencheetah_metadata.all_metadata_clean` a
--WHERE
--   a.id = '{4ed32d19-aeb6-43c1-aa11-7c41a1e935ee}.json' 
GROUP BY
   a.id,
   a.yob,
   a.gender
"
# run the query
metadata_all_summary <- query_exec(query, projectid, use_legacy_sql = FALSE)
```
Now that the data is in a smaller dataframe, it requires further summarisation and feature generation. This can be run on local machine. For the scope of this project I exported the file from the Google AI Notebook and imported it into my local machine.
```{r}
setwd('/Users/Andrew/Dropbox/Studies/MSC Big Data Analytics/Dissertation/IT Artefact')
metadata_all_summary <- read.csv(file = 'metadata_all_summary_2.csv',sep = ",")
# some of the summarised data lines have null values, these cant be used in the model and therefore need to be removed
metadata_all_summary[complete.cases(metadata_all_summary),] # omit lines with missing data
metadata_all_summary <- na.omit(metadata_all_summary) # remove NA
# there seem to be some errors with birth dates being greater then the current year, therefore only athletes over the age of 16 will be included. This data is up to date to 2020, therefore birth dates > than 2020-16 will be omitted, people older than 100 will be omitted. 
# male will be converted to 1 and female = 0
# drop athletes that have less than 30 rides where dropped, as this would roughly give one month of data per athlete that is included
# drop athletes that dont have a max critical power number for 60m as this is required to test against FTP
# create calculated eftp from best 20min x 95%
# remove all fields that are to do with watts per kg as these are calculated off the the max critical values/weight, they would therefore be highly correlated
library(sqldf)
metadata_all_summary <- sqldf(
                              "
                              Select
                                    ID,
                                    case when gender = 'M' then 1 else 0 end as gender,
                                    2020-yob as age,
                                    max_critical_power_60m*0.95 as eFTP,
                                    --no_of_rides,
                                    --avg_workout_time,
                                    --avg_time_riding,
                                    avg_weight,
                                    avg_power_all_rides,
                                    --avg_nonzero_power_all_rides,
                                    --max_power,
                                    --best_cp_setting,
                                    avg_coggan_np,
                                    --avg_coggan_variability_index,
                                    max_critical_power_1s,	
                                    max_critical_power_5s,
                                    max_critical_power_10s,
                                    max_critical_power_15s,	
                                    max_critical_power_20s,
                                    max_critical_power_30s,	
                                    max_critical_power_1m,
                                    max_critical_power_2m,
                                    max_critical_power_3m,
                                    max_critical_power_5m,
                                    max_critical_power_8m,
                                    max_critical_power_10m,
                                    max_critical_power_20m,
                                    max_critical_power_30m,
                                    max_critical_power_60m,
                                    avg_time_in_zone_L1/(avg_time_in_zone_L1+avg_time_in_zone_L2+
                                                         avg_time_in_zone_L3+avg_time_in_zone_L4+
                                                         avg_time_in_zone_L5+avg_time_in_zone_L6+
                                                         avg_time_in_zone_L7) as perc_zone1,
                                    avg_time_in_zone_L2/(avg_time_in_zone_L1+avg_time_in_zone_L2+
                                                         avg_time_in_zone_L3+avg_time_in_zone_L4+
                                                         avg_time_in_zone_L5+avg_time_in_zone_L6+
                                                         avg_time_in_zone_L7) as perc_zone2,
                                    avg_time_in_zone_L3/(avg_time_in_zone_L1+avg_time_in_zone_L2+
                                                         avg_time_in_zone_L3+avg_time_in_zone_L4+
                                                         avg_time_in_zone_L5+avg_time_in_zone_L6+
                                                         avg_time_in_zone_L7) as perc_zone3,
                                    avg_time_in_zone_L4/(avg_time_in_zone_L1+avg_time_in_zone_L2+
                                                         avg_time_in_zone_L3+avg_time_in_zone_L4+
                                                         avg_time_in_zone_L5+avg_time_in_zone_L6+
                                                         avg_time_in_zone_L7) as perc_zone4,
                                    avg_time_in_zone_L5/(avg_time_in_zone_L1+avg_time_in_zone_L2+
                                                         avg_time_in_zone_L3+avg_time_in_zone_L4+
                                                         avg_time_in_zone_L5+avg_time_in_zone_L6+
                                                         avg_time_in_zone_L7) as perc_zone5,
                                    avg_time_in_zone_L6/(avg_time_in_zone_L1+avg_time_in_zone_L2+
                                                         avg_time_in_zone_L3+avg_time_in_zone_L4+
                                                         avg_time_in_zone_L5+avg_time_in_zone_L6+
                                                         avg_time_in_zone_L7) as perc_zone6,
                                    avg_time_in_zone_L7/(avg_time_in_zone_L1+avg_time_in_zone_L2+
                                                         avg_time_in_zone_L3+avg_time_in_zone_L4+
                                                         avg_time_in_zone_L5+avg_time_in_zone_L6+
                                                         avg_time_in_zone_L7) as perc_zone7
                              From
                                    metadata_all_summary 
                              Where
                                    no_of_rides >= 30
                                    and max_critical_power_60m > 50*0.98
                                    -- we used 50w*0.98 due to the lowest value on the power profile (pg 41) for woman being w/p/kg at 0.98w, and we chose to only include riders above 50kg's for this analysis
                              "
                              )
# next is variable reduction, this is the most important part
library(data.table)
metadata_all_summary <-  as.data.table(metadata_all_summary)
metadata_all_summary <- setkey(metadata_all_summary,id)
str(metadata_all_summary)
# drop eFTP to small
# drop eFTP to high
library("ggplot2")
# compute a histogram of `metadata_all_summary$eFTP` to understand distribution
# from the distribution delete figures that seem to low and ones that seem to high
qplot(metadata_all_summary$eFTP, geom="histogram") 
qplot(metadata_all_summary$eFTP,
      geom="histogram",
      binwidth = 25,  
      main = "Histogram for eFTP", 
      xlab = "eFTP",  
      fill=I("blue"), 
      col=I("red"), 
      alpha=I(.2),
      xlim=c(0,1000))


```
Some more checking and data cleaning
```{r}
qplot(metadata_all_summary$eFTP,
      geom="histogram",
      binwidth = 50,  
      main = "Histogram for eFTP", 
      xlab = "eFTP",  
      fill=I("blue"), 
      col=I("red"), 
      alpha=I(.2),
      xlim=c(0,600))

summary(metadata_all_summary)
# after reviewing this it gives a far more realistic view of the athletes what we would like to study
# check if there are any NA's
sum(is.na(metadata_all_summary))
# there are 70, lets check what the data looks like
metadata_na <- metadata_all_summary[rowSums(is.na(metadata_all_summary)) > 0, ]
# these athletes have too much missing data and therefore will be dropped. There are only 10 of them
metadata_all_summary <- metadata_all_summary[complete.cases(metadata_all_summary), ]
# convert all to numeric
str(metadata_all_summary)
metadata_all_summary$age <- as.numeric(metadata_all_summary$age)
metadata_all_summary$gender <- as.numeric(metadata_all_summary$gender)
str(metadata_all_summary[ , c(2:29)])
```
Creating a histogram of % of FTP witnessed versus best 20min max effort
```{r}
metadata_summary_test <-
  sqldf("select 
                *,
                max_critical_power_60m/max_critical_power_20m as actual_ftp_perc
  
        from metadata_all_summary
        ")
library("ggplot2")
qplot(metadata_summary_test$actual_ftp_perc,
      geom="histogram",
      binwidth = 0.01,  
      main = "Histogram for actual_FTP_perc relative to best max 20-minute number", 
      xlab = "actual_ftp_perc",  
      fill=I("blue"), 
      col=I("red"), 
      alpha=I(.2),
      xlim=c(0.5,1))

```
Creating classification buckets (this was not used)
Looking for outliers with the use of histograms
```{r}
# this was never used to as a model as a classification model was not used, but rather a linear regression. The buckets where created to allow for a classification model if a classification model was to be used.
# buckets need to be created from this bucket to make it easier for a model to classify
# 1st iteration create buckets of 2%
# https://www.jdatalab.com/data_science_and_data_mining/2017/01/30/data-binning-plot.html

buckets <- as.data.frame(metadata_summary_test$actual_ftp_perc)
names(buckets)[names(buckets) == 'metadata_summary_test$actual_ftp_perc'] <- 'actual_ftp_perc'
ggplot(data = metadata_summary_test, mapping = aes(x=actual_ftp_perc)) + 
  geom_histogram(aes(y=..density..),fill="bisque",color="white",alpha=0.7) + 
  geom_density() +
  geom_rug() +
  labs(x='actual_ftp_perc') +
  theme_minimal()

# set up cut-off values 
breaks <- c(0,
            0.75,
            0.775,
            0.8,
            0.825,
            0.85,
            0.875,
            0.9,
            0.925,
            0.95,
            0.975,
            1)
# specify interval/bin labels
tags <- c("[0-0.75)",
          "[0.75-0.775)",
          "[0.775-0.8)",
          "[0.8-0.825)",
          "[0.825-0.85)",
          "[0.85-0.875)",
          "[0.875-0.9)", 
          "[0.9-0.925)",
          "[0.925-0.95)",
          "[0.95-0.975)",
          "[0.975-1)")
# bucketing values into bins
group_tags <- cut(buckets$actual_ftp_perc, 
                               breaks=breaks, 
                               include.lowest=TRUE, 
                               right=FALSE, 
                               labels=tags)
# inspect bins
summary(group_tags)

buckets$actual_ftp_bin <- cut(buckets$actual_ftp_perc, 
                               breaks=breaks, 
                               include.lowest=TRUE, 
                               right=FALSE, 
                               labels=tags)

metadata_all_summary <- cbind(metadata_all_summary,buckets)

# checking for outliers
colNames <- names(metadata_all_summary[ , c(2:29)])
for(i in colNames){
  plt <- ggplot(data=metadata_all_summary, aes_string(x=i)) +
  geom_histogram(aes(y =..density..), fill = "orange") +
  geom_density()
  print(plt)
  Sys.sleep(2)
}
#install.packages('tidyverse')
#library(tidyverse)
#ggplot(data = as_tibble(group_tags),
#       mapping = aes(x=value)) + 
#       geom_bar(fill="bisque",color="white",alpha=0.7) + 
#       stat_count(geom="text", aes(label=sprintf("%.4f",..count../length(group_tags))), vjust=-0.5) +
#       labs(x='actual_ftp_perc') +
#       theme(text = element_text(size=12),
#       axis.text.x = element_text(angle=90, hjust=1))
 

```
```{r}
# create linear regression dataset
# need to exclude 
lr_data <- subset(metadata_all_summary, select = -c(id,eFTP,actual_ftp_perc,actual_ftp_bin,max_critical_power_1s))
# check the distribution of the target variable
library(ggplot2)
# building FTP histogram
ggplot(data=lr_data, aes(lr_data$max_critical_power_60m)) +
  geom_histogram(aes(y =..density..), fill = "orange") +
  geom_density()
# target variable follows a normal distribution
# descriptive statistics of all variables being used in LR
install.packages('psych')
library(psych)
psych::describe(lr_data)
data_stats <- psych::describe(lr_data)

```
```{r}
# Next step is to check for correlation, this can be reviewed at a later stage once the model is developed to understand what correlation there is between the independent variables
install.packages("corrplot")
require(corrplot)
correlation <- cor(lr_data) 
correlation <- round(correlation,2)
# In theory, the correlation between the independent variables should be zero. In practice, we expect and are okay with weak to no correlation between independent variables.
# however we expect there to be correlation between all power variables
# split the data
```
Running Model 1
```{r}
install.packages('caret')
library(caret)
# Split data into train and test
index <- createDataPartition(lr_data$max_critical_power_60m, p = .70, list = FALSE)
train <- lr_data[index, ]
test <- lr_data[-index, ]
# Checking the dim of train
dim(train)

# building the model
# training model 1
lmModel <- lm(max_critical_power_60m ~ . , data = train)
# Printing the model object
print(lmModel)
# Checking model statistics
summary(lmModel)
# Using AIC function
AIC(lmModel)
# Using BIC function
BIC(lmModel)  
# 1st iteration
# AIC [1] 19937.58
# BIC [1] 20088.33
```
Running Model 2
```{r}
#running model with significant variables only
lr_data_sig <- subset(lr_data, select = c(avg_weight,
                                          avg_power_all_rides,
                                          avg_coggan_np,
                                          max_critical_power_5s,
                                          max_critical_power_10s,
                                          max_critical_power_30m,
                                          perc_zone1,
                                          perc_zone3,
                                          max_critical_power_60m))
# Split data into train and test
index <- createDataPartition(lr_data_sig$max_critical_power_60m, p = .70, list = FALSE)
train_1 <- lr_data_sig[index, ]
test_1 <- lr_data_sig[-index, ]
# Checking the dim of train
dim(train_1)

# building the model
# training model 2
lmModel_1 <- lm(max_critical_power_60m ~ . , data = train_1)
# Printing the model object
print(lmModel_1)
# Checking model statistics
summary(lmModel_1)
# Using AIC function
AIC(lmModel_1)
# Using BIC function
BIC(lmModel_1)  
```
Running Model 3
```{r}
lr_data_sig_2 <- subset(lr_data, select = c(avg_weight,
                                           avg_power_all_rides,
                                           max_critical_power_30m,
                                           perc_zone1,
                                           perc_zone3,
                                           max_critical_power_60m))
# Split data into train and test
index <- createDataPartition(lr_data_sig_2$max_critical_power_60m, p = .70, list = FALSE)
train_2 <- lr_data_sig_2[index, ]
test_2 <- lr_data_sig_2[-index, ]
# Checking the dim of train
dim(train_2)

# building the model
# training model 3
lmModel_2 <- lm(max_critical_power_60m ~ . , data = train_2)
# Printing the model object
print(lmModel_2)
# Checking model statistics
summary(lmModel_2)

# Using AIC function
AIC(lmModel_2)
# Using BIC function
BIC(lmModel_2)  

# 3rd iteration
# AIC [1] 20209.61
# BIC [1] 20250.19
```
Residual diagnostics of Model 2
```{r}
names(lmModel_1)

install.packages("Metrics")
library(Metrics)
rmse(actual = train_1$max_critical_power_60m, predicted = lmModel_1$fitted.values)

# residual diagnostics
# 1. determine mean of zero
mean(lmModel_1$residuals)
# 1.209694e-15 is extremely close to zero therefore satisfying this condition

# 2. Normality of residuals
install.packages("olsrr")
library(olsrr)
ols_plot_resid_hist(lmModel_1)
ols_plot_resid_fit(lmModel_1)

#perform Breusch-Pagan Test
library(lmtest)
bptest(lmModel_1)

# Histogram to check the distribution of errors
hist(lmModel_1$residuals, 
     color = "grey",
     breaks = 50)

```
There seems to be Heteroscedasticity present, to solve for this we will weight certain variable
Running Model 4 to weight variables and try fix heteroscedasticity
```{r}
#define weights to use
wt <- 1 / lm(abs(lmModel_1$residuals) ~ lmModel_1$fitted.values)$fitted.values^2

#perform weighted least squares regression
wls_model <- lm(max_critical_power_60m ~ . , data = train_1, weights=wt)


#view summary of model
summary(wls_model)

# Using AIC function
AIC(wls_model)
# Using BIC function
BIC(wls_model) 

ols_plot_resid_hist(wls_model)
ols_plot_resid_fit(wls_model)
bptest(wls_model)
hist(wls_model$residuals, 
     color = "grey",
     breaks = 50)

```
Running Model 5 to weight variables and try fix heteroscedasticity with only significant variables from Model 4
```{r}
lr_data_sig_3 <- subset(lr_data, select = c(avg_power_all_rides,
                                            max_critical_power_5s,
                                           max_critical_power_30m,
                                           perc_zone3,
                                           max_critical_power_20m,
                                           max_critical_power_60m))
# Split data into train and test
index <- createDataPartition(lr_data_sig_3$max_critical_power_60m, p = .70, list = FALSE)
train_3 <- lr_data_sig_3[index, ]
test_3 <- lr_data_sig_3[-index, ]
# Checking the dim of train
dim(train_3)

# building the model
# training model 3
lmModel_3 <- lm(max_critical_power_60m ~ avg_power_all_rides + 
                                         max_critical_power_5s +
                                         max_critical_power_30m +
                                         perc_zone3, 
               data = train_3)

# Printing the model object
print(lmModel_3)
# Checking model statistics
summary(lmModel_3)

# Using AIC function
AIC(lmModel_3)
# Using BIC function
BIC(lmModel_3)  

ols_plot_resid_hist(lmModel_3)
ols_plot_resid_fit(lmModel_3)
bptest(lmModel_3)


#define weights to use
wt_2 <- 1 / lm(abs(lmModel_3$residuals) ~ lmModel_3$fitted.values)$fitted.values^2

#perform weighted least squares regression
wls_model_2 <- lm(max_critical_power_60m ~ avg_power_all_rides + 
                                           max_critical_power_5s +
                                           max_critical_power_30m +
                                           perc_zone3,
                 data = train_3, weights=wt_2)


# Printing the model object
print(wls_model_2)
#view summary of model
summary(wls_model_2)

# Using AIC function
AIC(wls_model_2)
# Using BIC function
BIC(wls_model_2) 

ols_plot_resid_hist(wls_model_2)
ols_plot_resid_fit(wls_model_2)
bptest(wls_model_2)
hist(wls_model_2$residuals, 
     color = "grey",
     breaks = 50)
```

```{r}
# we except the points to be very close to the dotted line in an NPP plot. Points being close to the line means that errors follow a normal distribution.
plot(wls_model_2)
# see 2nd diagram for NPP plot if the dots follow the line then its show normal distribution
```
There should be no multicollinearity – The linear model assumes that the predictor variables do not correlate with each other. If they exhibit high correlation, it is a problem and is called multicollinearity. A variation inflation factor test can help check for the multicollinearity assumption.

VIF = 1/(1-R2)

The R implementation of the below function can be found here.

VIF is an iterative process. The function will remove one variable at a time, which is cause for multicollinearity and repeats the process until all problem causing variables are removed. So, finally, we are left with the list of variables that have no or very weak correlation between them.


There should be no auto serial correlation – The autocorrelation means that error terms should not be correlated with each other. To check this, we can run the Durbin-Watson test(dw test). The test returns a value between 0 and 4. If the value is two, we say there is no auto serial correlation. However, a value higher than 2 represents (-) ve correlation and value lower than 2 represents (+) ve correlation.

```{r}
#install.packages("lmtest")
library("lmtest")
#dwtest(wls_model_2)
ols_vif_tol(wls_model_2)
#	Durbin-Watson test

# alternative hypothesis: true autocorrelation is greater than 0
# Durbin-Watson’s d tests the null hypothesis that the residuals are not linearly auto-correlated.  While d can assume values between 0 and 4, values around 2 indicate no autocorrelation.  As a rule of thumb values of 1.5 < d < 2.5 show that there is no auto-correlation in the data.
```
Now its time to test the test set to validate the accuracy.

Predicting Dependent Variable(Y) in Test dataset
We test the model performance on test data set to ensure that our model is stable, and we get the same or closer enough results to use this trained model to predict and forecast future values of dependent variables. To predict, we use predict function, and then we generate R-Squared value to see if we get the same result as we got in the training dataset or not.

```{r}
# Predicting pFTP in test dataset
test_3$predicted_max_critical_power_60m <- predict(wls_model_2, test_3)
# Priting top 6 rows of actual and predited price
head(test_1[ , c("max_critical_power_60m", "predicted_max_critical_power_60m")])

# Generating r^2 of test dataset
# Predicting 60min max in test dataset
test_3$predicted_max_critical_power_60m <- predict(wls_model_2, test_3)
actual <- test_3$max_critical_power_60m
preds <- test_3$predicted_max_critical_power_60m
rss <- sum((preds - actual) ^ 2)
tss <- sum((actual - mean(actual)) ^ 2)
rsq <- 1 - rss/tss
rsq
```
The final step would be to add the eFTP value to the test_3 data to compare the FTP, predicted FTP and eFTP
```{r}
final_test <- cbind(test_3,test_3$max_critical_power_20m*0.95)
library(dplyr)
final_test <- rename(final_test, eFTP = V2)
str(final_test)
```
graph the actual vs predicted values in R
1st aFTP vs pFTP
2nd aFTP vs eFTP

https://statisticsglobe.com/plot-predicted-vs-actual-values-in-r
```{r}
library("ggplot2")

#actual vs predicted
ggplot(final_test, # Draw plot using ggplot2 package
       aes(x = predicted_max_critical_power_60m,
           y = max_critical_power_60m)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red",
              size = 2)+
       xlim(0,500)+
       ylim(0,500)

#actual vs eFTP
ggplot(final_test, # Draw plot using ggplot2 package
       aes(x = eFTP,
           y = max_critical_power_60m)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red",
              size = 2)+
       xlim(0,500)+
       ylim(0,500)


#write.csv(final_test,"\\Users\\Andrew\\Dropbox\\Studies\\MSC Big Data Analytics\\Dissertation\\IT Artefact\\final_test_2.csv", row.names = TRUE)

#write.csv(metadata_all_summary,"\\Users\\Andrew\\Dropbox\\Studies\\MSC Big Data Analytics\\Dissertation\\IT Artefact\\metadata_all_summary.csv", row.names = TRUE)

#write.csv(lr_data,"\\Users\\Andrew\\Dropbox\\Studies\\MSC Big Data Analytics\\Dissertation\\IT Artefact\\lr_data.csv", row.names = TRUE)

#write.csv(test,"\\Users\\Andrew\\Dropbox\\Studies\\MSC Big Data Analytics\\Dissertation\\IT Artefact\\test.csv", row.names = TRUE)
```
Calculate descriptive stats
```{r}
library(psych)

desc_stat_aFTP <- describe(final_test$max_critical_power_60m) 
write.csv(desc_stat_aFTP,"\\Users\\Andrew\\Dropbox\\Studies\\MSC Big Data Analytics\\Dissertation\\IT Artefact\\desc_stat_aFTP.csv", row.names = TRUE)

desc_stat_pFTP <- describe(final_test$predicted_max_critical_power_60m) 
write.csv(desc_stat_pFTP,"\\Users\\Andrew\\Dropbox\\Studies\\MSC Big Data Analytics\\Dissertation\\IT Artefact\\desc_stat_pFTP.csv", row.names = TRUE)

desc_stat_20m <- describe(final_test$max_critical_power_20m) 
write.csv(desc_stat_20m,"\\Users\\Andrew\\Dropbox\\Studies\\MSC Big Data Analytics\\Dissertation\\IT Artefact\\desc_stat_20m.csv", row.names = TRUE)

desc_stat_eFTP <- describe(final_test$eFTP) 
write.csv(desc_stat_eFTP,"\\Users\\Andrew\\Dropbox\\Studies\\MSC Big Data Analytics\\Dissertation\\IT Artefact\\desc_stat_eFTP.csv", row.names = TRUE)
```

calc MSE (Mean Squared Error)
calc MAE (Mean Absolute Error)
```{r}
#two was to calc MSE & MAE, the second way was used
#first way
#fit regression model
lr_mse_pftp <- lm(max_critical_power_60m~predicted_max_critical_power_60m, data=final_test)
lr_mse_eftp <- lm(max_critical_power_60m~eFTP, data=final_test)

#get model summary
lr_mse_pftp_summ <-summary(lr_mse_pftp)
lr_mse_eftp_summ <-summary(lr_mse_eftp)

summary(lr_mse_pftp)
summary(lr_mse_eftp)

#calculate MSE
print("MSE - aFTP vs pFTP")
mean(lr_mse_pftp_summ$residuals^2)
print("MSE - aFTP vs eFTP")
mean(lr_mse_eftp_summ$residuals^2)

#second way
print("MSE - aFTP vs pFTP")
mean((final_test$max_critical_power_60m - final_test$predicted_max_critical_power_60m)^2)
print("MSE - aFTP vs eFTP")
mean((final_test$max_critical_power_60m - final_test$eFTP)^2)

print("MAE - aFTP vs pFTP")
mean(abs(final_test$max_critical_power_60m - final_test$predicted_max_critical_power_60m))
print("MAE - aFTP vs eFTP")
mean(abs(final_test$max_critical_power_60m - final_test$eFTP))

#final_test_2 <- cbind(final_test,
#                     (final_test$max_critical_power_60m - final_test$predicted_max_critical_power_60m)^2,
#                     (final_test$max_critical_power_60m - final_test$eFTP)^2,
#                     abs(final_test$max_critical_power_60m - final_test$predicted_max_critical_power_60m),
#                     abs(final_test$max_critical_power_60m - final_test$eFTP))

```


