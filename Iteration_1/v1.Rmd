---
title: "Msc Dissertation"
output: html_notebook
---

Download the data
```{r}
# final edit
# download all metadata json files into google cloud storage bucket
library("plyr")
library("dplyr")
library("aws.s3")
library("purrr")
library("stringr")

setwd("/home/jupyter/goldencheetah_data")

Athlete_metadata <- get_bucket_df(bucket = "goldencheetah-opendata", prefix = "metadata", max = Inf) %>% pull(Key)

Athlete_metadata <- strsplit(Athlete_metadata, "\r\n")
Athlete_metadata = unlist(Athlete_metadata)
#str(Athlete_metadata)
#length(Athlete_metadata)
#Athlete_metadata[2:6607]
atm <- length(Athlete_metadata)
str_subset(Athlete_metadata[2:atm], "\\.zip$") %>%
  walk(function(key) {
    filename <- str_extract(key, "\\{.+")
    save_object(object = key, bucket = "goldencheetah-opendata", file = filename)
  })

t1 <- Sys.time()
# get all the zip files
zipF <- list.files(path = "/home/jupyter/goldencheetah_data", pattern = "*.zip", full.names = TRUE)

# unzip all your files
ldply(.data = zipF, .fun = unzip, exdir = "/home/jupyter/goldencheetah_data_unzipp")
t2 <- Sys.time()
print('Time to download and extract all files')
print(t2-t1)
```

Parse all the files into a bigquery table

```{r}
require(jsonlite)
require(data.table)
require(foreach)
require(doParallel)
# Get data into a single list

setwd("/home/jupyter/goldencheetah_data_unzipp/")
filenames <- list.files(path = "/home/jupyter/goldencheetah_data_unzipp/")
idx_bike <- list() # if we want to filter for "Bike" observations

registerDoParallel(cores = 16)
# registerDoSEQ()
t1 <- Sys.time()
idx_bike <- foreach(f = 1:6605, .combine = c, .multicombine = TRUE, .errorhandling = "remove") %dopar% {
  test <- try({
    which(fromJSON(txt=filenames[f])$RIDES$sport == "Bike" |
          fromJSON(txt=filenames[f])$RIDES$sport == "VirtualRide")
  })
  list(test)
}
t2 <- Sys.time()
paste0("time to run bike index loop")
print(t2 - t1)

dt_merged <- list()

dt_merged <- foreach(f = 4001:6605, .combine = c, .multicombine = TRUE, .errorhandling = "remove") %dopar% {
  test <- try({
    dt_merged <- fromJSON(txt=filenames[f])$RIDES$METRICS[idx_bike[[f]],]
    dt_merged[["date"]] <- fromJSON(txt=filenames[f])$RIDES$date[idx_bike[[f]]]
    dt_merged[["id"]] <-  as.list(filenames[f]) #rep(fromJSON(txt=filenames[f])$ATHLETE$id, length(dt_merged[[f]][["date"]]))
    dt_merged[["sport"]] <- fromJSON(txt=filenames[f])$RIDES$sport[idx_bike[[f]]]
    dt_merged[["yob"]] <-  fromJSON(txt=filenames[f])$ATHLETE$yob
    dt_merged[["gender"]] <-  fromJSON(txt=filenames[f])$ATHLETE$gender
    })
  list(dt_merged)
}

t3 <- Sys.time()
paste0("time to run dt_merged loop")
print(t3 - t2)



# Merge into a data.table
dt_merged <- rbindlist(dt_merged, fill = TRUE)
#head(dt_merged)

# Reorder columns such that "id", "date", "sport", "yob" and "gender" are the first columns
idx_metadata_cols <- which(colnames(dt_merged) %in% c("date","id","sport","yob","gender"))
idx_metric_cols <- which(!colnames(dt_merged) %in% c("date","id","sport","yob","gender"))
dt_merged <- dt_merged[,c(idx_metadata_cols,idx_metric_cols), with=FALSE]

#set all NA to 0 that are not in a list
#dt_merged[is.na(dt_merged)] <- 0
#dt_merged[is.null(dt_merged)] <- 0

idx_list <- which(sapply(dt_merged, class) == "list")
#idx_list

# select only columns required
dt_merged <- dt_merged[,c('date','id','sport','gender','yob','ride_count','workout_time','time_riding','athlete_weight','total_work','average_power','nonzero_power',
                          'max_power','cp_setting','coggan_np','coggan_if','coggan_tss','coggam_variability_index','coggan_tssperhour',
                          '1s_critical_power','5s_critical_power','10s_critical_power','15s_critical_power','20s_critical_power',
                          '30s_critical_power','1m_critical_power','2m_critical_power','3m_critical_power','5m_critical_power','8m_critical_power',
                          '10m_critical_power','20m_critical_power','30m_critical_power','60m_critical_power','time_in_zone_L1','time_in_zone_L2',
                          'time_in_zone_L3','time_in_zone_L4','time_in_zone_L5','time_in_zone_L6','time_in_zone_L7','1s_peak_wpk','5s_peak_wpk',
                          '10s_peak_wpk','15s_peak_wpk','20s_peak_wpk','30s_peak_wpk','1m_peak_wpk','5m_peak_wpk','10m_peak_wpk','20m_peak_wpk',
                          '30m_peak_wpk','60m_peak_wpk')]

# only include rows that have an average power value where a 1sec power value is not null
dt_merged <- dt_merged[!sapply(dt_merged$average_power, is.null)]
dt_merged <- dt_merged[!sapply(dt_merged$'1s_peak_wpk', is.null)]

# fix list columns that have null values
# remove rows with "NULL" values
zero_idx <- which(dt_merged$average_power=="NULL")
dt_merged$average_power[zero_idx] <- 0
dt_merged$average_power <- as.numeric(unlist(lapply(dt_merged$average_power, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$coggan_np=="NULL")
dt_merged$coggan_np[zero_idx] <- 0
dt_merged$coggan_np <- as.numeric(unlist(lapply(dt_merged$coggan_np, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$coggan_if=="NULL")
dt_merged$coggan_if[zero_idx] <- 0
dt_merged$coggan_if <- as.numeric(unlist(lapply(dt_merged$coggan_if, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$nonzero_power=="NULL")
dt_merged$nonzero_power[zero_idx] <- 0
dt_merged$nonzero_power <- as.numeric(unlist(lapply(dt_merged$nonzero_power, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$coggam_variability_index=="NULL")
dt_merged$coggam_variability_index[zero_idx] <- 0
dt_merged$coggam_variability_index <- as.numeric(unlist(lapply(dt_merged$coggam_variability_index, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$coggan_tssperhour=="NULL")
dt_merged$coggan_tssperhour[zero_idx] <- 0
dt_merged$coggan_tssperhour <- as.numeric(unlist(lapply(dt_merged$coggan_tssperhour, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$time_in_zone_L1=="NULL")
dt_merged$time_in_zone_L1[zero_idx] <- 0
dt_merged$time_in_zone_L1 <- as.numeric(unlist(lapply(dt_merged$time_in_zone_L1, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$time_in_zone_L2=="NULL")
dt_merged$time_in_zone_L2[zero_idx] <- 0
dt_merged$time_in_zone_L2 <- as.numeric(unlist(lapply(dt_merged$time_in_zone_L2, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$time_in_zone_L3=="NULL")
dt_merged$time_in_zone_L3[zero_idx] <- 0
dt_merged$time_in_zone_L3 <- as.numeric(unlist(lapply(dt_merged$time_in_zone_L3, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$time_in_zone_L4=="NULL")
dt_merged$time_in_zone_L4[zero_idx] <- 0
dt_merged$time_in_zone_L4 <- as.numeric(unlist(lapply(dt_merged$time_in_zone_L4, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$time_in_zone_L5=="NULL")
dt_merged$time_in_zone_L5[zero_idx] <- 0
dt_merged$time_in_zone_L5 <- as.numeric(unlist(lapply(dt_merged$time_in_zone_L5, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$time_in_zone_L6=="NULL")
dt_merged$time_in_zone_L6[zero_idx] <- 0
dt_merged$time_in_zone_L6 <- as.numeric(unlist(lapply(dt_merged$time_in_zone_L6, `[[`, 1)))
# remove rows with "NULL" values
zero_idx <- which(dt_merged$time_in_zone_L7=="NULL")
dt_merged$time_in_zone_L7[zero_idx] <- 0
dt_merged$time_in_zone_L7 <- as.numeric(unlist(lapply(dt_merged$time_in_zone_L7, `[[`, 1)))

# change column types
#dt_merged$date <- as.POSIXct(dt_merged$date, "%Y-%m-%d %H:%M:%S")
dt_merged$ride_count <- as.numeric(dt_merged$ride_count)
dt_merged$workout_time <- as.numeric(dt_merged$workout_time)
dt_merged$time_riding <- as.numeric(dt_merged$time_riding)
dt_merged$athlete_weight <- as.numeric(dt_merged$athlete_weight)
dt_merged$total_work <- as.numeric(dt_merged$total_work)
dt_merged$max_power <- as.numeric(dt_merged$max_power)
dt_merged$cp_setting <- as.numeric(dt_merged$cp_setting)
dt_merged$coggan_tss <- as.numeric(dt_merged$coggan_tss)
dt_merged$'1s_critical_power' <- as.numeric(dt_merged$'1s_critical_power')
dt_merged$'5s_critical_power' <- as.numeric(dt_merged$'5s_critical_power')
dt_merged$'10s_critical_power' <- as.numeric(dt_merged$'10s_critical_power')
dt_merged$'15s_critical_power' <- as.numeric(dt_merged$'15s_critical_power')
dt_merged$'20s_critical_power' <- as.numeric(dt_merged$'20s_critical_power')
dt_merged$'30s_critical_power' <- as.numeric(dt_merged$'30s_critical_power')
dt_merged$'1m_critical_power' <- as.numeric(dt_merged$'1m_critical_power')
dt_merged$'2m_critical_power' <- as.numeric(dt_merged$'2m_critical_power')
dt_merged$'3m_critical_power' <- as.numeric(dt_merged$'3m_critical_power')
dt_merged$'5m_critical_power' <- as.numeric(dt_merged$'5m_critical_power')
dt_merged$'8m_critical_power' <- as.numeric(dt_merged$'8m_critical_power')
dt_merged$'10m_critical_power' <- as.numeric(dt_merged$'10m_critical_power')
dt_merged$'20m_critical_power' <- as.numeric(dt_merged$'20m_critical_power')
dt_merged$'30m_critical_power' <- as.numeric(dt_merged$'30m_critical_power')
dt_merged$'60m_critical_power' <- as.numeric(dt_merged$'60m_critical_power')
dt_merged$time_in_zone_L1 <- as.numeric(dt_merged$time_in_zone_L1)
dt_merged$time_in_zone_L2 <- as.numeric(dt_merged$time_in_zone_L2)
dt_merged$time_in_zone_L3 <- as.numeric(dt_merged$time_in_zone_L3)
dt_merged$time_in_zone_L4 <- as.numeric(dt_merged$time_in_zone_L4)
dt_merged$time_in_zone_L5 <- as.numeric(dt_merged$time_in_zone_L5)
dt_merged$time_in_zone_L6 <- as.numeric(dt_merged$time_in_zone_L6)
dt_merged$time_in_zone_L7 <- as.numeric(dt_merged$time_in_zone_L7)
dt_merged$'1s_peak_wpk' <- as.numeric(dt_merged$'1s_peak_wpk')
dt_merged$'5s_peak_wpk' <- as.numeric(dt_merged$'5s_peak_wpk')
dt_merged$'10s_peak_wpk' <- as.numeric(dt_merged$'10s_peak_wpk')
dt_merged$'15s_peak_wpk' <- as.numeric(dt_merged$'15s_peak_wpk')
dt_merged$'20s_peak_wpk' <- as.numeric(dt_merged$'20s_peak_wpk')
dt_merged$'30s_peak_wpk' <- as.numeric(dt_merged$'30s_peak_wpk')
dt_merged$'1m_peak_wpk' <- as.numeric(dt_merged$'1m_peak_wpk')
dt_merged$'5m_peak_wpk' <- as.numeric(dt_merged$'5m_peak_wpk')
dt_merged$'10m_peak_wpk' <- as.numeric(dt_merged$'10m_peak_wpk')
dt_merged$'20m_peak_wpk' <- as.numeric(dt_merged$'20m_peak_wpk')
dt_merged$'30m_peak_wpk' <- as.numeric(dt_merged$'30m_peak_wpk')
dt_merged$'60m_peak_wpk' <- as.numeric(dt_merged$'60m_peak_wpk')
#class(dt_merged$coggan_tssperhour)
# rename columns that start with a number for BigQuery upload
setnames(dt_merged, old=c('1s_critical_power'), new=c('critical_power_1s'))
setnames(dt_merged, old=c('5s_critical_power'), new=c('critical_power_5s'))
setnames(dt_merged, old=c('10s_critical_power'), new=c('critical_power_10s'))
setnames(dt_merged, old=c('15s_critical_power'), new=c('critical_power_15s'))
setnames(dt_merged, old=c('20s_critical_power'), new=c('critical_power_20s'))
setnames(dt_merged, old=c('30s_critical_power'), new=c('critical_power_30s'))
setnames(dt_merged, old=c('1m_critical_power'), new=c('critical_power_1m'))
setnames(dt_merged, old=c('2m_critical_power'), new=c('critical_power_2m'))
setnames(dt_merged, old=c('3m_critical_power'), new=c('critical_power_3m'))
setnames(dt_merged, old=c('5m_critical_power'), new=c('critical_power_5m'))
setnames(dt_merged, old=c('8m_critical_power'), new=c('critical_power_8m'))
setnames(dt_merged, old=c('10m_critical_power'), new=c('critical_power_10m'))
setnames(dt_merged, old=c('20m_critical_power'), new=c('critical_power_20m'))
setnames(dt_merged, old=c('30m_critical_power'), new=c('critical_power_30m'))
setnames(dt_merged, old=c('60m_critical_power'), new=c('critical_power_60m'))
setnames(dt_merged, old=c('1s_peak_wpk'), new=c('peak_wpk_1s'))
setnames(dt_merged, old=c('5s_peak_wpk'), new=c('peak_wpk_5s'))
setnames(dt_merged, old=c('10s_peak_wpk'), new=c('peak_wpk_10s'))
setnames(dt_merged, old=c('15s_peak_wpk'), new=c('peak_wpk_15s'))
setnames(dt_merged, old=c('20s_peak_wpk'), new=c('peak_wpk_20s'))
setnames(dt_merged, old=c('30s_peak_wpk'), new=c('peak_wpk_30s'))
setnames(dt_merged, old=c('1m_peak_wpk'), new=c('peak_wpk_1m'))
setnames(dt_merged, old=c('5m_peak_wpk'), new=c('peak_wpk_5m'))
setnames(dt_merged, old=c('10m_peak_wpk'), new=c('peak_wpk_10m'))
setnames(dt_merged, old=c('20m_peak_wpk'), new=c('peak_wpk_20m'))
setnames(dt_merged, old=c('30m_peak_wpk'), new=c('peak_wpk_30m'))
setnames(dt_merged, old=c('60m_peak_wpk'), new=c('peak_wpk_60m'))

dt_merged$id <- unlist(dt_merged$id)

#idx_list <- which(sapply(dt_merged, class)=="list") # this is empty, implying no more list types

t4 <- Sys.time()
paste0('Time to subset list elements, convert data types')
print(t4-t3)
  

    
#write to bigquery
t3 <- Sys.time()
library(bigQueryR)
# use this to upload data to bigquery table
bqr_auth()
bqr_upload_data(projectId = "xxx", 
                datasetId = "goldencheetah_metadata",#name of gcp projectid
                tableId = "all_metadata_4001_6605", #name of the table 
                upload_data = dt_merged) #where the data is coming from
t4 <- Sys.time()
paste0('Uploaded to BigQuery')
print(t3-t4)
```
Imported exported bigquery data into R
convert to datatable and summarise all fields
```{r}
# clean data from power meter errors and unrealistic numbers
library(bigrquery)
projectid <- "algebraic-pier-304102"
query1 <-
"
create or replace table `algebraic-pier-304102.goldencheetah_metadata.all_metadata_clean` as
-- drop rides that have numbers that out of realm of possibility based on the table found on pg 41 (power profile chart)
-- max values will come from the men
-- athlete min age 16
-- athlete max age 100
-- weight between 50-150kgs
SELECT  
    *
FROM 
    `algebraic-pier-304102.goldencheetah_metadata.all_metadata` a
WHERE
    a.critical_power_5s/athlete_weight <= 25.18 --and 7.09
    and a.critical_power_1m/athlete_weight <= 11.5 --and 4.1
    and a.critical_power_5m/athlete_weight <= 7.6 --and 1.29
    and a.critical_power_60m/athlete_weight <= 6.6 --and 0.98
    and a.athlete_weight between 50 and 150
    and cast(a.yob as numeric) between 1920 and 2004
"
# Run the query
metadata_all_clean <- query_exec(query1, projectid, use_legacy_sql = FALSE)

```


```{r}
library(bigrquery)
projectid <- "xxx"
query <-
"
create table `algebraic-pier-304102.goldencheetah_metadata.all_metadata_summary` as
SELECT
    a.id,
    a.gender,
    a.yob,
    round(sum(ride_count),2) as no_of_rides,
    round(avg(workout_time),2) as avg_workout_time,
    round(avg(time_riding),2) as avg_time_riding,
    round(avg(athlete_weight),2) as avg_weight,
    round(avg(a.average_power),2) as avg_power_all_rides,
    round(avg(a.nonzero_power),2) as avg_nonzero_power_all_rides,
    round(max(max_power),2) as max_power,
    round(max(cp_setting),2) as best_cp_setting,
    round(avg(coggan_np),2) as avg_coggan_np,
    round(avg(coggam_variability_index),2) as avg_coggan_variability_index,
    round(max(critical_power_1s),2) as max_critical_power_1s,	
    round(max(critical_power_5s),2) as max_critical_power_5s,
    round(max(critical_power_10s),2) as max_critical_power_10s,
    round(max(critical_power_15s),2) as max_critical_power_15s,	
    round(max(critical_power_20s),2) as max_critical_power_20s,
    round(max(critical_power_30s),2) as max_critical_power_30s,	
    round(max(critical_power_1m),2) as max_critical_power_1m,
    round(max(critical_power_2m),2) as max_critical_power_2m,
    round(max(critical_power_3m),2) as max_critical_power_3m,
    round(max(critical_power_5m),2) as max_critical_power_5m,
    round(max(critical_power_8m),2) as max_critical_power_8m,
    round(max(critical_power_10m),2) as max_critical_power_10m,
    round(max(critical_power_20m),2) as max_critical_power_20m,
    round(max(critical_power_30m),2) as max_critical_power_30m,
    round(max(critical_power_60m),2) as max_critical_power_60m,
    round(avg(time_in_zone_L1),2) as avg_time_in_zone_L1,
    round(avg(time_in_zone_L2),2) as avg_time_in_zone_L2,
    round(avg(time_in_zone_L3),2) as avg_time_in_zone_L3,
    round(avg(time_in_zone_L4),2) as avg_time_in_zone_L4,
    round(avg(time_in_zone_L5),2) as avg_time_in_zone_L5,
    round(avg(time_in_zone_L6),2) as avg_time_in_zone_L6,
    round(avg(time_in_zone_L7),2) as avg_time_in_zone_L7,
    round(max(peak_wpk_1s),2) as max_peak_wpk_1s,
    round(max(peak_wpk_5s),2) as max_peak_wpk_5s,
    round(max(peak_wpk_10s),2) as max_peak_wpk_10s,	
    round(max(peak_wpk_15s),2) as max_peak_wpk_15s,	
    round(max(peak_wpk_20s),2) as max_peak_wpk_20s,	
    round(max(peak_wpk_30s),2) as max_peak_wpk_30s,	
    round(max(peak_wpk_1m),2) as max_peak_wpk_1m,
    round(max(peak_wpk_5m),2) as max_peak_wpk_5m,
    round(max(peak_wpk_10m),2) as max_peak_wpk_10m,	
    round(max(peak_wpk_20m),2) as max_peak_wpk_20m,	
    round(max(peak_wpk_30m),2) as max_peak_wpk_30m,	
    round(max(peak_wpk_60m),2) as max_peak_wpk_60m
    

FROM 
    `algebraic-pier-304102.goldencheetah_metadata.all_metadata_clean` a
--WHERE
--   a.id = '{4ed32d19-aeb6-43c1-aa11-7c41a1e935ee}.json' 
GROUP BY
   a.id,
   a.yob,
   a.gender
"
# Run the query
metadata_all_summary <- query_exec(query, projectid, use_legacy_sql = FALSE)
```
Now that the data is in a smaller dataframe, it requires further summarisation and feature generation
```{r}
setwd('/Users/Andrew/Dropbox/Studies/MSC Big Data Analytics/Dissertation/IT Artefact')
metadata_all_summary <- read.csv(file = 'metadata_all_summary_2.csv',sep = ",")
# some of the summarised data lines have null values, these cant be used in the model and therefore need to be removed
metadata_all_summary[complete.cases(metadata_all_summary),] # omit lines with missing data
metadata_all_summary <- na.omit(metadata_all_summary) # Remove NA
# there seem to be some errors with birth dates being greater then the current year, therefore only athletes over the age of 16 will be included. This data is up to date to 2020, therefore birth dates > than 2020-16 will be omitted, people older than 100 will be omitted. 
# male will be converted to and female = 0
# drop athletes that have less than 30 rides where dropped, as this would roughly give one month of data per athlete that is included
# drop athletes that dont have a max critical power number for 60m as this is required to test against FTP
# create calculated eftp from best 20min x 95%
# remove all fields that are to do with watts per kg as these are calculated off the the max critical values/weight, they would therefore be highly correlated
library(sqldf)
metadata_all_summary <- sqldf(
                              "
                              Select
                                    ID,
                                    case when gender = 'M' then 1 else 0 end as gender,
                                    2020-yob as age,
                                    max_critical_power_60m*0.95 as eFTP,
                                    --no_of_rides,
                                    --avg_workout_time,
                                    --avg_time_riding,
                                    avg_weight,
                                    avg_power_all_rides,
                                    --avg_nonzero_power_all_rides,
                                    --max_power,
                                    --best_cp_setting,
                                    avg_coggan_np,
                                    --avg_coggan_variability_index,
                                    max_critical_power_1s,	
                                    max_critical_power_5s,
                                    max_critical_power_10s,
                                    max_critical_power_15s,	
                                    max_critical_power_20s,
                                    max_critical_power_30s,	
                                    max_critical_power_1m,
                                    max_critical_power_2m,
                                    max_critical_power_3m,
                                    max_critical_power_5m,
                                    max_critical_power_8m,
                                    max_critical_power_10m,
                                    max_critical_power_20m,
                                    max_critical_power_30m,
                                    max_critical_power_60m,
                                    avg_time_in_zone_L1/(avg_time_in_zone_L1+avg_time_in_zone_L2+
                                                         avg_time_in_zone_L3+avg_time_in_zone_L4+
                                                         avg_time_in_zone_L5+avg_time_in_zone_L6+
                                                         avg_time_in_zone_L7) as perc_zone1,
                                    avg_time_in_zone_L2/(avg_time_in_zone_L1+avg_time_in_zone_L2+
                                                         avg_time_in_zone_L3+avg_time_in_zone_L4+
                                                         avg_time_in_zone_L5+avg_time_in_zone_L6+
                                                         avg_time_in_zone_L7) as perc_zone2,
                                    avg_time_in_zone_L3/(avg_time_in_zone_L1+avg_time_in_zone_L2+
                                                         avg_time_in_zone_L3+avg_time_in_zone_L4+
                                                         avg_time_in_zone_L5+avg_time_in_zone_L6+
                                                         avg_time_in_zone_L7) as perc_zone3,
                                    avg_time_in_zone_L4/(avg_time_in_zone_L1+avg_time_in_zone_L2+
                                                         avg_time_in_zone_L3+avg_time_in_zone_L4+
                                                         avg_time_in_zone_L5+avg_time_in_zone_L6+
                                                         avg_time_in_zone_L7) as perc_zone4,
                                    avg_time_in_zone_L5/(avg_time_in_zone_L1+avg_time_in_zone_L2+
                                                         avg_time_in_zone_L3+avg_time_in_zone_L4+
                                                         avg_time_in_zone_L5+avg_time_in_zone_L6+
                                                         avg_time_in_zone_L7) as perc_zone5,
                                    avg_time_in_zone_L6/(avg_time_in_zone_L1+avg_time_in_zone_L2+
                                                         avg_time_in_zone_L3+avg_time_in_zone_L4+
                                                         avg_time_in_zone_L5+avg_time_in_zone_L6+
                                                         avg_time_in_zone_L7) as perc_zone6,
                                    avg_time_in_zone_L7/(avg_time_in_zone_L1+avg_time_in_zone_L2+
                                                         avg_time_in_zone_L3+avg_time_in_zone_L4+
                                                         avg_time_in_zone_L5+avg_time_in_zone_L6+
                                                         avg_time_in_zone_L7) as perc_zone7
                              From
                                    metadata_all_summary 
                              Where
                                    no_of_rides >= 30
                                    and max_critical_power_60m > 50*0.98
                                    -- we used 50w*0.98 due to the lowest value on the power profile (pg 41) for woman being w/p/kg at 0.98w, and we chose to only include riders above 50kg's for this analysis
                              "
                              )
# Next is variable reduction, this is the most important part
library(data.table)
metadata_all_summary <-  as.data.table(metadata_all_summary)
metadata_all_summary <- setkey(metadata_all_summary,id)
str(metadata_all_summary)
# drop eFTP to small
# drop eFTP to high
library("ggplot2")
# Compute a histogram of `metadata_all_summary$eFTP` to understand distribution
# from the distribution delete figures that seem to low and ones that seem to high
qplot(metadata_all_summary$eFTP, geom="histogram") 
qplot(metadata_all_summary$eFTP,
      geom="histogram",
      binwidth = 25,  
      main = "Histogram for eFTP", 
      xlab = "eFTP",  
      fill=I("blue"), 
      col=I("red"), 
      alpha=I(.2),
      xlim=c(0,1000))


```
Remove athletes the eFTP > 6.7w per kg

```{r}
qplot(metadata_all_summary$eFTP,
      geom="histogram",
      binwidth = 50,  
      main = "Histogram for eFTP", 
      xlab = "eFTP",  
      fill=I("blue"), 
      col=I("red"), 
      alpha=I(.2),
      xlim=c(0,600))

summary(metadata_all_summary)
# after reviewing this it gives a far more realistic view of the athletes what we would like to study
# check if there are any NA's
sum(is.na(metadata_all_summary))
# there are 70, lets check what the data looks like
meta_data_na <- metadata_all_summary[rowSums(is.na(metadata_all_summary)) > 0, ]
# these athletes have too much missing data and therefore will be dropped. There are only 10 of them
metadata_all_summary <- metadata_all_summary[complete.cases(metadata_all_summary), ]

```

The next portion was not run in a jupyter notebook, but rather Rstudio, as the final output file was now small enough to run on local machine. It was therefore export to Google Cloud Storage in CSV format, downloaded onto my local machine and uploaded in Rstudio
```{r}
install.packages("fpc")
install.packages("dbscan")
install.packages("factoextra")

# Compute DBSCAN using fpc package
library("fpc")
set.seed(123)
dbscan <- fpc::dbscan(metadata_all_summary, eps = 0.15, MinPts = 5)

```
Example area
```{r}
summary(metadata_all_summary)


```
looking for outliers

```{r}
# remove all 20min 
metadata_all_summary <-
  sqldf(
      "
      Select
            *
      From
            metadata_all_summary 
      wherea
            max_critical_power_20m /avg_weight < 6.71/0.95
      "
      )
```

